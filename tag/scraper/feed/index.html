<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>scraper &#8211; PageOneX</title>
	<atom:link href="https://blog.pageonex.com/tag/scraper/feed/" rel="self" type="application/rss+xml" />
	<link>https://blog.pageonex.com</link>
	<description>Coding front pages</description>
	<lastBuildDate>Fri, 06 Jul 2012 01:32:41 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.4.2</generator>
	<item>
		<title>Arab Spring Visualization with Processing + Inkscape</title>
		<link>https://blog.pageonex.com/2012/07/06/arab-spring-visualization-with-processing-inkscape/</link>
					<comments>https://blog.pageonex.com/2012/07/06/arab-spring-visualization-with-processing-inkscape/#comments</comments>
		
		<dc:creator><![CDATA[pablo]]></dc:creator>
		<pubDate>Fri, 06 Jul 2012 01:32:41 +0000</pubDate>
				<category><![CDATA[Scraping]]></category>
		<category><![CDATA[kiosko]]></category>
		<category><![CDATA[processing]]></category>
		<category><![CDATA[scraper]]></category>
		<guid isPermaLink="false">http://montera34.org/pageonex/?p=156</guid>

					<description><![CDATA[I&#8217;ve done this datavis on arab spring with processing + inkscape and here&#8217;s Pablo&#8217;s comments In order to see the highlighted areas front pages need to be less intense (semitransparent). US newspapers have longer format. We&#8217;ll have to take that in account. Every newspaper has its own format. Preserve it. Highlighted areas need to be stronger. Use multiply option in the fusion of the layer, and not transparency. I think you changed from layer after you coded the &#8220;mubarak photo&#8221; 6.3 nytimes, so the&#8230; <a href="https://blog.pageonex.com/2012/07/06/arab-spring-visualization-with-processing-inkscape/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
		
					<wfw:commentRss>https://blog.pageonex.com/2012/07/06/arab-spring-visualization-with-processing-inkscape/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>Update Scraping script to scrape from many sources</title>
		<link>https://blog.pageonex.com/2012/06/20/update-scraping-script-to-scrape-from-many-sources/</link>
					<comments>https://blog.pageonex.com/2012/06/20/update-scraping-script-to-scrape-from-many-sources/#comments</comments>
		
		<dc:creator><![CDATA[pablo]]></dc:creator>
		<pubDate>Wed, 20 Jun 2012 04:18:43 +0000</pubDate>
				<category><![CDATA[code]]></category>
		<category><![CDATA[Scraping]]></category>
		<category><![CDATA[elpais]]></category>
		<category><![CDATA[kiosko]]></category>
		<category><![CDATA[nytimes]]></category>
		<category><![CDATA[ruby]]></category>
		<category><![CDATA[scraper]]></category>
		<guid isPermaLink="false">http://montera34.org/pageonex/?p=47</guid>

					<description><![CDATA[I&#8217;ve made some changes to the script to scrape from different sources (http://kiosko.net, http:/nytimes.com, http://elpais.com) and other sources can be added easily, for each source there are two methods, build_source_issues and save_source_issues, the first method is to construct the URI of the issue image based on some pattern which different from source to another, and the other method is to scrape the images and save them on the disk in their specific folders. I&#8217;ve wrote some comments to clear some parts of&#8230; <a href="https://blog.pageonex.com/2012/06/20/update-scraping-script-to-scrape-from-many-sources/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
		
					<wfw:commentRss>https://blog.pageonex.com/2012/06/20/update-scraping-script-to-scrape-from-many-sources/feed/</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>First Scraping script for Kiosko.net</title>
		<link>https://blog.pageonex.com/2012/06/13/first-scrapping-script-for-kiosko-net/</link>
					<comments>https://blog.pageonex.com/2012/06/13/first-scrapping-script-for-kiosko-net/#respond</comments>
		
		<dc:creator><![CDATA[pablo]]></dc:creator>
		<pubDate>Wed, 13 Jun 2012 20:06:11 +0000</pubDate>
				<category><![CDATA[code]]></category>
		<category><![CDATA[Scraping]]></category>
		<category><![CDATA[kiosko]]></category>
		<category><![CDATA[newseum]]></category>
		<category><![CDATA[ruby]]></category>
		<category><![CDATA[scraper]]></category>
		<guid isPermaLink="false">http://montera34.org/pageonex/?p=31</guid>

					<description><![CDATA[Ahmd has been working on a scrapper in Ruby for the front Pages at Kiosko.net I&#8217;ve finished the scraping script, and it&#8217;s public on https://gist.github.com/2925910 to run the script just pass the file to ruby [ruby scraper.rb] and it will generate the folders (the directories is set for Linux, if you are on Windows you should modify them first), download the images(you can change the variable values in the get_issues method to get different newspapers), and write the&#8230; <a href="https://blog.pageonex.com/2012/06/13/first-scrapping-script-for-kiosko-net/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
		
					<wfw:commentRss>https://blog.pageonex.com/2012/06/13/first-scrapping-script-for-kiosko-net/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
